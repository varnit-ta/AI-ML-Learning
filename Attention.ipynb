{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import random\n",
    "from numpy import dot\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder representations of four different words\n",
    "''' \n",
    "In actual practice, these word embeddings would \n",
    "have been generated by an encoder; however, for this \n",
    "particular example, you will define them manually. \n",
    "'''\n",
    "word_1 = array([1, 0, 0])\n",
    "word_2 = array([0, 1, 0])\n",
    "word_3 = array([1, 1, 0])\n",
    "word_4 = array([0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking the word embeddings into a single array\n",
    "words = array([word_1, word_2, word_3, word_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the weight matrices\n",
    "'''\n",
    "The next step generates the weight matrices, which \n",
    "you will eventually multiply to the word embeddings \n",
    "to generate the queries, keys, and values.\n",
    "'''\n",
    "random.seed(40)\n",
    "W_Q = random.randint(3, size=(3, 3))\n",
    "W_K = random.randint(3, size=(3, 3))\n",
    "W_V = random.randint(3, size=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the queries, keys and values\n",
    "'''\n",
    "Subsequently, the query, key, and value vectors for each \n",
    "word are generated by multiplying each word embedding by \n",
    "each of the weight matrices. \n",
    "'''\n",
    "Q = words @ W_Q\n",
    "K = words @ W_K\n",
    "V = words @ W_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring the query vectors against all key vectors\n",
    "'''\n",
    "The next step scores its query vector against all the key \n",
    "vectors using a dot product operation. \n",
    "'''\n",
    "scores = Q @ K.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the weights by a softmax operation\n",
    "'''\n",
    "The score values are subsequently passed through a softmax operation \n",
    "to generate the weights. Before doing so, it is common practice to divide \n",
    "the score values by the square root of the dimensionality of the key \n",
    "vectors (in this case, three) to keep the gradients stable. \n",
    "'''\n",
    "weights = softmax(scores / K.shape[1] ** 0.5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the attention by a weighted sum of the value vectors\n",
    "'''\n",
    "Finally, the attention output is calculated by a weighted sum of all \n",
    "four value vectors. \n",
    "'''\n",
    "attention = weights @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.68198596 1.68198596 1.57598128]\n",
      " [1.16552931 1.16552931 0.41477798]\n",
      " [1.50216109 1.50216109 1.05426852]\n",
      " [1.08547961 1.08547961 0.22483734]]\n"
     ]
    }
   ],
   "source": [
    "print(attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
