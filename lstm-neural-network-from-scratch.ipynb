{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7651,"sourceType":"datasetVersion","datasetId":13}],"dockerImageVersionId":3200,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Implementing LSTM Neural Network from Scratch\n**Dataset = Us Baby's First Names <br>\nLink = https://www.kaggle.com/chr7stos/us-names-babies-and-presindent-names/data**","metadata":{"_uuid":"2d19769d3d823d95e24e3e5a549030117159de4f"}},{"cell_type":"markdown","source":"### Import Required Libraries","metadata":{"_uuid":"53caf20b4edf5889ebdd350cefb9c6c8ccd397db"}},{"cell_type":"code","source":"import numpy as np               #for maths\nimport pandas as pd              #for data manipulation\nimport matplotlib.pyplot as plt  #for visualization","metadata":{"_uuid":"03273aed79e22c299518292e5720f4954df13537","execution":{"iopub.status.busy":"2024-06-14T06:12:51.875195Z","iopub.execute_input":"2024-06-14T06:12:51.875579Z","iopub.status.idle":"2024-06-14T06:12:51.882999Z","shell.execute_reply.started":"2024-06-14T06:12:51.875507Z","shell.execute_reply":"2024-06-14T06:12:51.881905Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"###  Load the data","metadata":{"_uuid":"1b8c6a0c424b4ce91338f8cd49cdf45c863d1c1f"}},{"cell_type":"code","source":"#data \npath = r'../input/NationalNames.csv'\ndata = pd.read_csv(path)\n\n#get names from the dataset\ndata['Name'] = data['Name']\n\n#get first 10000 names\ndata = np.array(data['Name'][:10000]).reshape(-1,1)\n\n#covert the names to lowee case\ndata = [x.lower() for x in data[:,0]]\n\ndata = np.array(data).reshape(-1,1)","metadata":{"_uuid":"a81dfbf915bbe5f7aeddde8f56031e8395386483","execution":{"iopub.status.busy":"2024-06-14T06:12:51.884560Z","iopub.execute_input":"2024-06-14T06:12:51.884949Z","iopub.status.idle":"2024-06-14T06:12:54.269841Z","shell.execute_reply.started":"2024-06-14T06:12:51.884883Z","shell.execute_reply":"2024-06-14T06:12:54.268908Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(\"Data Shape = {}\".format(data.shape))\nprint()\nprint(\"Lets see some names : \")\nprint(data[1:10])","metadata":{"_uuid":"7092768c5890de7fff72ed4f2a96a03f0e0dd543","execution":{"iopub.status.busy":"2024-06-14T06:12:54.271185Z","iopub.execute_input":"2024-06-14T06:12:54.271493Z","iopub.status.idle":"2024-06-14T06:12:54.279010Z","shell.execute_reply.started":"2024-06-14T06:12:54.271440Z","shell.execute_reply":"2024-06-14T06:12:54.278101Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Data Shape = (10000, 1)\n\nLets see some names : \n[['anna']\n ['emma']\n ['elizabeth']\n ['minnie']\n ['margaret']\n ['ida']\n ['alice']\n ['bertha']\n ['sarah']]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Transform the names to equal length by adding -- > ('.')  dots","metadata":{"_uuid":"21682a8a6a0644a3f4533755c5d47a3946f647b6"}},{"cell_type":"code","source":"#to store the transform data\ntransform_data = np.copy(data)\n\n#find the max length name\nmax_length = 0\nfor index in range(len(data)):\n    max_length = max(max_length,len(data[index,0]))\n\n#make every name of max length by adding '.'\nfor index in range(len(data)):\n    length = (max_length - len(data[index,0]))\n    string = '.'*length\n    transform_data[index,0] = ''.join([transform_data[index,0],string])","metadata":{"_uuid":"e6e1a392fbb163fb199d9a1f9855618b67c384de","execution":{"iopub.status.busy":"2024-06-14T06:12:54.280487Z","iopub.execute_input":"2024-06-14T06:12:54.280944Z","iopub.status.idle":"2024-06-14T06:12:54.341616Z","shell.execute_reply.started":"2024-06-14T06:12:54.280859Z","shell.execute_reply":"2024-06-14T06:12:54.340654Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(\"Transformed Data\")\nprint(transform_data[1:10])","metadata":{"_uuid":"169aa04299734a309faf4b2517ced8dc458d8e2a","execution":{"iopub.status.busy":"2024-06-14T06:12:54.343032Z","iopub.execute_input":"2024-06-14T06:12:54.343351Z","iopub.status.idle":"2024-06-14T06:12:54.349156Z","shell.execute_reply.started":"2024-06-14T06:12:54.343292Z","shell.execute_reply":"2024-06-14T06:12:54.348297Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Transformed Data\n[['anna........']\n ['emma........']\n ['elizabeth...']\n ['minnie......']\n ['margaret....']\n ['ida.........']\n ['alice.......']\n ['bertha......']\n ['sarah.......']]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Lets Make Vocabulary","metadata":{"_uuid":"fc16433b33308855028e3437fe16de90636fc9cd"}},{"cell_type":"code","source":"#to store the vocabulary\nvocab = list()\nfor name in transform_data[:,0]:\n    vocab.extend(list(name))\n\nvocab = set(vocab)\nvocab_size = len(vocab)\n\nprint(\"Vocab size = {}\".format(len(vocab)))\nprint(\"Vocab      = {}\".format(vocab))","metadata":{"_uuid":"56b3e746aec5bef31f4d076723189d3a3de3bb91","execution":{"iopub.status.busy":"2024-06-14T06:12:54.350333Z","iopub.execute_input":"2024-06-14T06:12:54.350638Z","iopub.status.idle":"2024-06-14T06:12:54.380647Z","shell.execute_reply.started":"2024-06-14T06:12:54.350582Z","shell.execute_reply":"2024-06-14T06:12:54.379554Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Vocab size = 27\nVocab      = {'i', 'o', 'c', 's', '.', 'n', 'p', 'x', 'j', 'm', 'g', 'e', 'd', 'w', 'u', 'r', 'z', 'q', 'h', 'y', 'k', 'l', 't', 'a', 'v', 'f', 'b'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Map characters to ids and ids to characters","metadata":{"_uuid":"57d10787e793e123757d863c447e2cc0b505ce40"}},{"cell_type":"code","source":"#map char to id and id to chars\nchar_id = dict()\nid_char = dict()\n\nfor i,char in enumerate(vocab):\n    char_id[char] = i\n    id_char[i] = char\n\nprint('a-{}, 22-{}'.format(char_id['a'],id_char[22]))","metadata":{"_uuid":"dacb673988043fb503aaf3b356aacb1a2f923ab2","execution":{"iopub.status.busy":"2024-06-14T06:12:54.382004Z","iopub.execute_input":"2024-06-14T06:12:54.382303Z","iopub.status.idle":"2024-06-14T06:12:54.394225Z","shell.execute_reply.started":"2024-06-14T06:12:54.382246Z","shell.execute_reply":"2024-06-14T06:12:54.393368Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"a-23, 22-t\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Make the Train dataset\n\nExample - names - [['mary.'], ['anna.'] <br>\nm - [0,0,0,1,0,0] <br>\na - [0,0,1,0,0,0] <br>\nr - [0,1,0,0,0,0] <br>\ny - [0,0,0,0,1,0] <br>\n**.** - [1,0,0,0,0,0] <br>\n\n'mary.' = [[0,0,0,1,0,0], [0,0,1,0,0,0], [0,1,0,0,0,0], [0,0,0,0,1,0], [1,0,0,0,0,0]] <br>\n'anna.' = [[0,0,1,0,0,0], [0,0,0,0,0,1], [0,0,0,0,0,1], [0,0,1,0,0,0], [1,0,0,0,0,0]] <br>\n\nbatch_dataset = [ [[0,0,0,1,0,0],[0,0,1,0,0,0]] , [[0,0,1,0,0,0], [0,0,0,0,0,1]], [[0,1,0,0,0,0], [0,0,0,0,0,1]], [[0,0,0,0,1,0], [0,0,1,0,0,0]] , [ [1,0,0,0,0,0], [1,0,0,0,0,0]] ]","metadata":{"_uuid":"de643b041387eef2cbf512faa979e50ff9384116"}},{"cell_type":"code","source":"# list of batches of size = 20\ntrain_dataset = []\n\nbatch_size = 20\n\n#split the trasnform data into batches of 20\nfor i in range(len(transform_data)-batch_size+1):\n    start = i*batch_size\n    end = start+batch_size\n    \n    #batch data\n    batch_data = transform_data[start:end]\n    \n    if(len(batch_data)!=batch_size):\n        break\n        \n    #convert each char of each name of batch data into one hot encoding\n    char_list = []\n    for k in range(len(batch_data[0][0])):\n        batch_dataset = np.zeros([batch_size,len(vocab)])\n        for j in range(batch_size):\n            name = batch_data[j][0]\n            char_index = char_id[name[k]]\n            batch_dataset[j,char_index] = 1.0\n     \n        #store the ith char's one hot representation of each name in batch_data\n        char_list.append(batch_dataset)\n    \n    #store each char's of every name in batch dataset into train_dataset\n    train_dataset.append(char_list)","metadata":{"_uuid":"1563661dd644e744872a2f59f00349b4fe0bf93d","execution":{"iopub.status.busy":"2024-06-14T06:12:54.395474Z","iopub.execute_input":"2024-06-14T06:12:54.395991Z","iopub.status.idle":"2024-06-14T06:12:54.726325Z","shell.execute_reply.started":"2024-06-14T06:12:54.395902Z","shell.execute_reply":"2024-06-14T06:12:54.725565Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters","metadata":{"_uuid":"6514815063131615058ddcfa273025c43df19a82"}},{"cell_type":"code","source":"#number of input units or embedding size\ninput_units = 100\n\n#number of hidden neurons\nhidden_units = 256\n\n#number of output units i.e vocab size\noutput_units = vocab_size\n\n#learning rate\nlearning_rate = 0.005\n\n#beta1 for V parameters used in Adam Optimizer\nbeta1 = 0.90\n\n#beta2 for S parameters used in Adam Optimizer\nbeta2 = 0.99","metadata":{"_uuid":"2e1002eaa8120996b0fb060f7d62de46a35319bb","execution":{"iopub.status.busy":"2024-06-14T06:12:54.727614Z","iopub.execute_input":"2024-06-14T06:12:54.727931Z","iopub.status.idle":"2024-06-14T06:12:54.737673Z","shell.execute_reply.started":"2024-06-14T06:12:54.727882Z","shell.execute_reply":"2024-06-14T06:12:54.736813Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Activation Functions\n* **Sigmoid =  1/(1+exp(-X))** <br>\n* **Tanh    =  (exp(X) - exp(-X)) / (exp(X) + exp(X)) **\n* **Softmax =  exp(X)/(sum(exp(X),1))**\n<br>\n![](https://i.stack.imgur.com/o0JA0.png)\n\n","metadata":{"_uuid":"67ef5188422a6df1c27937f27396a158dc0ce081"}},{"cell_type":"code","source":"#Activation Functions\n#sigmoid\ndef sigmoid(X):\n    return 1/(1+np.exp(-X))\n\n#tanh activation\ndef tanh_activation(X):\n    return np.tanh(X)\n\n#softmax activation\ndef softmax(X):\n    exp_X = np.exp(X)\n    exp_X_sum = np.sum(exp_X,axis=1).reshape(-1,1)\n    exp_X = exp_X/exp_X_sum\n    return exp_X\n\n#derivative of tanh\ndef tanh_derivative(X):\n    return 1-(X**2)","metadata":{"_uuid":"71cad984f2f6adce6c40b4b309a7e14c0ce1c1c9","execution":{"iopub.status.busy":"2024-06-14T06:12:54.739176Z","iopub.execute_input":"2024-06-14T06:12:54.739547Z","iopub.status.idle":"2024-06-14T06:12:54.761154Z","shell.execute_reply.started":"2024-06-14T06:12:54.739484Z","shell.execute_reply":"2024-06-14T06:12:54.760230Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Initialize Parameters\nEmbeddings Size = 100  <br>\nHidden Units = 256  <br>\nTotal INPUT Weights = 100 + 256 = 356 <br>\n\n* **LSTM CELL Weights ** <br>\n    * Forget Gate Weights = {356,256}  <br>\n    * Input Gate Weights  = {356,256}  <br>\n    * Gate Gate Weights   = {356,256}  <br>\n    * Output Gate Weights = {356,256}  <br>\n<br>\n\n* **Output CELL Weights ** <br>\n    * Output Weights = {256,27} <br>\n\nStore these weights in parameters dictionary!","metadata":{"_uuid":"28879cb124e75962321d6cbdb461938a4301c5a1"}},{"cell_type":"code","source":"#initialize parameters\ndef initialize_parameters():\n    #initialize the parameters with 0 mean and 0.01 standard deviation\n    mean = 0\n    std = 0.01\n    \n    #lstm cell weights\n    forget_gate_weights = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n    input_gate_weights  = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n    output_gate_weights = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n    gate_gate_weights   = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n    \n    #hidden to output weights (output cell)\n    hidden_output_weights = np.random.normal(mean,std,(hidden_units,output_units))\n    \n    parameters = dict()\n    parameters['fgw'] = forget_gate_weights\n    parameters['igw'] = input_gate_weights\n    parameters['ogw'] = output_gate_weights\n    parameters['ggw'] = gate_gate_weights\n    parameters['how'] = hidden_output_weights\n    \n    return parameters\n","metadata":{"_uuid":"38a224730e5f0e1a1be9ad854b0bb4b6f5f32894","execution":{"iopub.status.busy":"2024-06-14T06:12:54.762312Z","iopub.execute_input":"2024-06-14T06:12:54.762616Z","iopub.status.idle":"2024-06-14T06:12:54.798599Z","shell.execute_reply.started":"2024-06-14T06:12:54.762561Z","shell.execute_reply":"2024-06-14T06:12:54.797642Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### LSTM CELL\n\n![](https://github.com/navjindervirdee/neural-networks/blob/master/Recurrent%20Neural%20Network/LSTM.JPG?raw=true) <br>\n\n**Equations**\n* fa = sigmoid(Wf x [xt,at-1]) <br>\n* ia = sigmoid(Wi x [xt,at-1]) <br>\n* ga = tanh(Wg x [xt,at-1]) \n* oa = sigmoid(Wo x [xt,at-1]) \n* ct = (fa x ct-1) + (ia x ga) \n* at = oa x tanh(ct) ","metadata":{"_uuid":"9a288454bfac89da006cba961c72e9904e5cd9c2"}},{"cell_type":"code","source":"#single lstm cell\ndef lstm_cell(batch_dataset, prev_activation_matrix, prev_cell_matrix, parameters):\n    #get parameters\n    fgw = parameters['fgw']\n    igw = parameters['igw']\n    ogw = parameters['ogw']\n    ggw = parameters['ggw']\n    \n    #concat batch data and prev_activation matrix\n    concat_dataset = np.concatenate((batch_dataset,prev_activation_matrix),axis=1)\n    \n    #forget gate activations\n    fa = np.matmul(concat_dataset,fgw)\n    fa = sigmoid(fa)\n    \n    #input gate activations\n    ia = np.matmul(concat_dataset,igw)\n    ia = sigmoid(ia)\n    \n    #output gate activations\n    oa = np.matmul(concat_dataset,ogw)\n    oa = sigmoid(oa)\n    \n    #gate gate activations\n    ga = np.matmul(concat_dataset,ggw)\n    ga = tanh_activation(ga)\n    \n    #new cell memory matrix\n    cell_memory_matrix = np.multiply(fa,prev_cell_matrix) + np.multiply(ia,ga)\n    \n    #current activation matrix\n    activation_matrix = np.multiply(oa, tanh_activation(cell_memory_matrix))\n    \n    #lets store the activations to be used in back prop\n    lstm_activations = dict()\n    lstm_activations['fa'] = fa\n    lstm_activations['ia'] = ia\n    lstm_activations['oa'] = oa\n    lstm_activations['ga'] = ga\n    \n    return lstm_activations,cell_memory_matrix,activation_matrix","metadata":{"_uuid":"3a9f7cef02e4ffd2ca690f0575e460473691cd28","execution":{"iopub.status.busy":"2024-06-14T06:12:54.799911Z","iopub.execute_input":"2024-06-14T06:12:54.800189Z","iopub.status.idle":"2024-06-14T06:12:54.873459Z","shell.execute_reply.started":"2024-06-14T06:12:54.800135Z","shell.execute_reply":"2024-06-14T06:12:54.872673Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Output Cell\n\nEquations \n* ot = W x at\n* ot = softmax(ot)","metadata":{"_uuid":"3227239971da97d91fff9849fea2cf359ebb8dad"}},{"cell_type":"code","source":"def output_cell(activation_matrix,parameters):\n    #get hidden to output parameters\n    how = parameters['how']\n    \n    #get outputs \n    output_matrix = np.matmul(activation_matrix,how)\n    output_matrix = softmax(output_matrix)\n    \n    return output_matrix","metadata":{"_uuid":"8e13f9b6abafc9d582098ad767e90e19bdd8fb20","execution":{"iopub.status.busy":"2024-06-14T06:12:54.874791Z","iopub.execute_input":"2024-06-14T06:12:54.875069Z","iopub.status.idle":"2024-06-14T06:12:54.890731Z","shell.execute_reply.started":"2024-06-14T06:12:54.875020Z","shell.execute_reply":"2024-06-14T06:12:54.889857Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Get corresponding embeddings for the batch dataset","metadata":{"_uuid":"3de27b1ad75c27211f52e3c39cdd6c616ab29a36"}},{"cell_type":"code","source":"def get_embeddings(batch_dataset,embeddings):\n    embedding_dataset = np.matmul(batch_dataset,embeddings)\n    return embedding_dataset","metadata":{"_uuid":"1de55aa62f5317dcd6adf6841b83c81962c0a646","execution":{"iopub.status.busy":"2024-06-14T06:12:54.892202Z","iopub.execute_input":"2024-06-14T06:12:54.892555Z","iopub.status.idle":"2024-06-14T06:12:54.902824Z","shell.execute_reply.started":"2024-06-14T06:12:54.892494Z","shell.execute_reply":"2024-06-14T06:12:54.902046Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Forward Propagation\n![](https://github.com/navjindervirdee/neural-networks/blob/master/Recurrent%20Neural%20Network/LSTMForward.JPG?raw=true)\n\nFunction returns the intermediate ativations in the respective caches:\n* LSTM Cache :- All lstm cell activation in every cell (fa,ia,ga,oa)\n* Activation Cache : All activation (a0,a1,a2..)\n* Cell Cache : All cell activations (c0,c1,c2..\n* Embedding cache : Embeddings of each batch (e0,e1,e2..)\n* Output Cache : All output (o1,o2,o3... )","metadata":{"_uuid":"6ff6b0614522b36694536cc325645a16ca290a78"}},{"cell_type":"code","source":"#forward propagation\ndef forward_propagation(batches,parameters,embeddings):\n    #get batch size\n    batch_size = batches[0].shape[0]\n    \n    #to store the activations of all the unrollings.\n    lstm_cache = dict()                 #lstm cache\n    activation_cache = dict()           #activation cache \n    cell_cache = dict()                 #cell cache\n    output_cache = dict()               #output cache\n    embedding_cache = dict()            #embedding cache \n    \n    #initial activation_matrix(a0) and cell_matrix(c0)\n    a0 = np.zeros([batch_size,hidden_units],dtype=np.float32)\n    c0 = np.zeros([batch_size,hidden_units],dtype=np.float32)\n    \n    #store the initial activations in cache\n    activation_cache['a0'] = a0\n    cell_cache['c0'] = c0\n    \n    #unroll the names\n    for i in range(len(batches)-1):\n        #get first first character batch\n        batch_dataset = batches[i]\n        \n        #get embeddings \n        batch_dataset = get_embeddings(batch_dataset,embeddings)\n        embedding_cache['emb'+str(i)] = batch_dataset\n        \n        #lstm cell\n        lstm_activations,ct,at = lstm_cell(batch_dataset,a0,c0,parameters)\n        \n        #output cell\n        ot = output_cell(at,parameters)\n        \n        #store the time 't' activations in caches\n        lstm_cache['lstm' + str(i+1)]  = lstm_activations\n        activation_cache['a'+str(i+1)] = at\n        cell_cache['c' + str(i+1)] = ct\n        output_cache['o'+str(i+1)] = ot\n        \n        #update a0 and c0 to new 'at' and 'ct' for next lstm cell\n        a0 = at\n        c0 = ct\n        \n    return embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache","metadata":{"_uuid":"b96b7f6ae58183c3691b1b8a096ccc3519116bad","execution":{"iopub.status.busy":"2024-06-14T06:12:54.904175Z","iopub.execute_input":"2024-06-14T06:12:54.904444Z","iopub.status.idle":"2024-06-14T06:12:55.001294Z","shell.execute_reply.started":"2024-06-14T06:12:54.904401Z","shell.execute_reply":"2024-06-14T06:12:55.000495Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Calculate the Loss, Perplexity, and Accuracy\n**Loss**\n* Loss at time t = -sum(Y x log(d) + (1-Y) x log(1-pred)))/m\n* Overall Loss = **∑**(Loss(t)) sum of all losses at each time step 't'\n\n**Perplexity **\n* Probability Product = **∏**(prob(pred_char)) for each char in name\n* Perplexity = (1/probability_product) ^ (1/n) where n in number of chars in name\n\n**Accuracy**\n* Accuracy(t) = (Y==predictions,axis=1) for all time steps\n* Accuracy = ((**∑**Acc(t))/batch_size)/n for all time steps, n is number of chars in name","metadata":{"_uuid":"ccece450fafd45cd88b68dea56b3ab15b9161b4d"}},{"cell_type":"code","source":"#calculate loss, perplexity and accuracy\ndef cal_loss_accuracy(batch_labels,output_cache):\n    loss = 0  #to sum loss for each time step\n    acc  = 0  #to sum acc for each time step \n    prob = 1  #probability product of each time step predicted char\n    \n    #batch size\n    batch_size = batch_labels[0].shape[0]\n    \n    #loop through each time step\n    for i in range(1,len(output_cache)+1):\n        #get true labels and predictions\n        labels = batch_labels[i]\n        pred = output_cache['o'+str(i)]\n        \n        prob = np.multiply(prob,np.sum(np.multiply(labels,pred),axis=1).reshape(-1,1))\n        loss += np.sum((np.multiply(labels,np.log(pred)) + np.multiply(1-labels,np.log(1-pred))),axis=1).reshape(-1,1)\n        acc  += np.array(np.argmax(labels,1)==np.argmax(pred,1),dtype=np.float32).reshape(-1,1)\n    \n    #calculate perplexity loss and accuracy\n    perplexity = np.sum((1/prob)**(1/len(output_cache)))/batch_size\n    loss = np.sum(loss)*(-1/batch_size)\n    acc  = np.sum(acc)/(batch_size)\n    acc = acc/len(output_cache)\n    \n    return perplexity,loss,acc","metadata":{"_uuid":"43318e7340569b2421b9299c8fb958a0092e5713","execution":{"iopub.status.busy":"2024-06-14T06:12:55.002812Z","iopub.execute_input":"2024-06-14T06:12:55.003176Z","iopub.status.idle":"2024-06-14T06:12:55.053746Z","shell.execute_reply.started":"2024-06-14T06:12:55.003114Z","shell.execute_reply":"2024-06-14T06:12:55.052616Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Calculate Output Cell Errors for each time step\n* Output Error Cache :- to store output error for each time step\n* Activation Error Cache : to store activation error for each time step","metadata":{"_uuid":"50f0382ea6d886c08b74c99f40c5fe4658d102f4"}},{"cell_type":"code","source":"#calculate output cell errors\ndef calculate_output_cell_error(batch_labels,output_cache,parameters):\n    #to store the output errors for each time step\n    output_error_cache = dict()\n    activation_error_cache = dict()\n    how = parameters['how']\n    \n    #loop through each time step\n    for i in range(1,len(output_cache)+1):\n        #get true and predicted labels\n        labels = batch_labels[i]\n        pred = output_cache['o'+str(i)]\n        \n        #calculate the output_error for time step 't'\n        error_output = pred - labels\n        \n        #calculate the activation error for time step 't'\n        error_activation = np.matmul(error_output,how.T)\n        \n        #store the output and activation error in dict\n        output_error_cache['eo'+str(i)] = error_output\n        activation_error_cache['ea'+str(i)] = error_activation\n        \n    return output_error_cache,activation_error_cache","metadata":{"_uuid":"bbc7499117d4dca92c550d5eae960d0b9bad2b32","execution":{"iopub.status.busy":"2024-06-14T06:12:55.055201Z","iopub.execute_input":"2024-06-14T06:12:55.055512Z","iopub.status.idle":"2024-06-14T06:12:55.086193Z","shell.execute_reply.started":"2024-06-14T06:12:55.055455Z","shell.execute_reply":"2024-06-14T06:12:55.085201Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Calculate Single LSTM CELL Error","metadata":{"_uuid":"9cc14608467e9fa9eba4b36c5e1d2d24a80852a8"}},{"cell_type":"code","source":"#calculate error for single lstm cell\ndef calculate_single_lstm_cell_error(activation_output_error,next_activation_error,next_cell_error,parameters,lstm_activation,cell_activation,prev_cell_activation):\n    #activation error =  error coming from output cell and error coming from the next lstm cell\n    activation_error = activation_output_error + next_activation_error\n    \n    #output gate error\n    oa = lstm_activation['oa']\n    eo = np.multiply(activation_error,tanh_activation(cell_activation))\n    eo = np.multiply(np.multiply(eo,oa),1-oa)\n    \n    #cell activation error\n    cell_error = np.multiply(activation_error,oa)\n    cell_error = np.multiply(cell_error,tanh_derivative(tanh_activation(cell_activation)))\n    #error also coming from next lstm cell \n    cell_error += next_cell_error\n    \n    #input gate error\n    ia = lstm_activation['ia']\n    ga = lstm_activation['ga']\n    ei = np.multiply(cell_error,ga)\n    ei = np.multiply(np.multiply(ei,ia),1-ia)\n    \n    #gate gate error\n    eg = np.multiply(cell_error,ia)\n    eg = np.multiply(eg,tanh_derivative(ga))\n    \n    #forget gate error\n    fa = lstm_activation['fa']\n    ef = np.multiply(cell_error,prev_cell_activation)\n    ef = np.multiply(np.multiply(ef,fa),1-fa)\n    \n    #prev cell error\n    prev_cell_error = np.multiply(cell_error,fa)\n    \n    #get parameters\n    fgw = parameters['fgw']\n    igw = parameters['igw']\n    ggw = parameters['ggw']\n    ogw = parameters['ogw']\n    \n    #embedding + hidden activation error\n    embed_activation_error = np.matmul(ef,fgw.T)\n    embed_activation_error += np.matmul(ei,igw.T)\n    embed_activation_error += np.matmul(eo,ogw.T)\n    embed_activation_error += np.matmul(eg,ggw.T)\n    \n    input_hidden_units = fgw.shape[0]\n    hidden_units = fgw.shape[1]\n    input_units = input_hidden_units - hidden_units\n    \n    #prev activation error\n    prev_activation_error = embed_activation_error[:,input_units:]\n    \n    #input error (embedding error)\n    embed_error = embed_activation_error[:,:input_units]\n    \n    #store lstm error\n    lstm_error = dict()\n    lstm_error['ef'] = ef\n    lstm_error['ei'] = ei\n    lstm_error['eo'] = eo\n    lstm_error['eg'] = eg\n    \n    return prev_activation_error,prev_cell_error,embed_error,lstm_error","metadata":{"_uuid":"2fb6e8f9cae63d99adb92c79fea9f6d3a69d7633","execution":{"iopub.status.busy":"2024-06-14T06:12:55.087845Z","iopub.execute_input":"2024-06-14T06:12:55.088194Z","iopub.status.idle":"2024-06-14T06:12:55.320611Z","shell.execute_reply.started":"2024-06-14T06:12:55.088131Z","shell.execute_reply":"2024-06-14T06:12:55.319712Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Calculate Output Cell Derivatives for each time step","metadata":{"_uuid":"ec190975e8b1df1ae7dd03d1c87630ef00f66797"}},{"cell_type":"code","source":"#calculate output cell derivatives\ndef calculate_output_cell_derivatives(output_error_cache,activation_cache,parameters):\n    #to store the sum of derivatives from each time step\n    dhow = np.zeros(parameters['how'].shape)\n    \n    batch_size = activation_cache['a1'].shape[0]\n    \n    #loop through the time steps \n    for i in range(1,len(output_error_cache)+1):\n        #get output error\n        output_error = output_error_cache['eo' + str(i)]\n        \n        #get input activation\n        activation = activation_cache['a'+str(i)]\n        \n        #cal derivative and summing up!\n        dhow += np.matmul(activation.T,output_error)/batch_size\n        \n    return dhow","metadata":{"_uuid":"30d5bbe3137c06c647a8f7c337a68449d680f526","execution":{"iopub.status.busy":"2024-06-14T06:12:55.322188Z","iopub.execute_input":"2024-06-14T06:12:55.322537Z","iopub.status.idle":"2024-06-14T06:12:55.351011Z","shell.execute_reply.started":"2024-06-14T06:12:55.322459Z","shell.execute_reply":"2024-06-14T06:12:55.349959Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Calculate LSTM CELL Derivatives for each time step","metadata":{"_uuid":"188c17e489c24436903b29683ba7c94ea7d072fd"}},{"cell_type":"code","source":"#calculate derivatives for single lstm cell\ndef calculate_single_lstm_cell_derivatives(lstm_error,embedding_matrix,activation_matrix):\n    #get error for single time step\n    ef = lstm_error['ef']\n    ei = lstm_error['ei']\n    eo = lstm_error['eo']\n    eg = lstm_error['eg']\n    \n    #get input activations for this time step\n    concat_matrix = np.concatenate((embedding_matrix,activation_matrix),axis=1)\n    \n    batch_size = embedding_matrix.shape[0]\n    \n    #cal derivatives for this time step\n    dfgw = np.matmul(concat_matrix.T,ef)/batch_size\n    digw = np.matmul(concat_matrix.T,ei)/batch_size\n    dogw = np.matmul(concat_matrix.T,eo)/batch_size\n    dggw = np.matmul(concat_matrix.T,eg)/batch_size\n    \n    #store the derivatives for this time step in dict\n    derivatives = dict()\n    derivatives['dfgw'] = dfgw\n    derivatives['digw'] = digw\n    derivatives['dogw'] = dogw\n    derivatives['dggw'] = dggw\n    \n    return derivatives","metadata":{"_uuid":"180778f81b4f2db577f7ab39fafa7ce7d0775518","execution":{"iopub.status.busy":"2024-06-14T06:12:55.352428Z","iopub.execute_input":"2024-06-14T06:12:55.352796Z","iopub.status.idle":"2024-06-14T06:12:55.394893Z","shell.execute_reply.started":"2024-06-14T06:12:55.352738Z","shell.execute_reply":"2024-06-14T06:12:55.393825Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Backward Propagation\n\n* Apply chain rule and calculate the errors for each time step\n* Store the deivatives in **derivatives** dict","metadata":{"_uuid":"0034c9d5bab9f30909d91c5d877ae255ded9ddca"}},{"cell_type":"code","source":"#backpropagation\ndef backward_propagation(batch_labels,embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache,parameters):\n    #calculate output errors \n    output_error_cache,activation_error_cache = calculate_output_cell_error(batch_labels,output_cache,parameters)\n    \n    #to store lstm error for each time step\n    lstm_error_cache = dict()\n    \n    #to store embeding errors for each time step\n    embedding_error_cache = dict()\n    \n    # next activation error \n    # next cell error  \n    #for last cell will be zero\n    eat = np.zeros(activation_error_cache['ea1'].shape)\n    ect = np.zeros(activation_error_cache['ea1'].shape)\n    \n    #calculate all lstm cell errors (going from last time-step to the first time step)\n    for i in range(len(lstm_cache),0,-1):\n        #calculate the lstm errors for this time step 't'\n        pae,pce,ee,le = calculate_single_lstm_cell_error(activation_error_cache['ea'+str(i)],eat,ect,parameters,lstm_cache['lstm'+str(i)],cell_cache['c'+str(i)],cell_cache['c'+str(i-1)])\n        \n        #store the lstm error in dict\n        lstm_error_cache['elstm'+str(i)] = le\n        \n        #store the embedding error in dict\n        embedding_error_cache['eemb'+str(i-1)] = ee\n        \n        #update the next activation error and next cell error for previous cell\n        eat = pae\n        ect = pce\n    \n    \n    #calculate output cell derivatives\n    derivatives = dict()\n    derivatives['dhow'] = calculate_output_cell_derivatives(output_error_cache,activation_cache,parameters)\n    \n    #calculate lstm cell derivatives for each time step and store in lstm_derivatives dict\n    lstm_derivatives = dict()\n    for i in range(1,len(lstm_error_cache)+1):\n        lstm_derivatives['dlstm'+str(i)] = calculate_single_lstm_cell_derivatives(lstm_error_cache['elstm'+str(i)],embedding_cache['emb'+str(i-1)],activation_cache['a'+str(i-1)])\n    \n    #initialize the derivatives to zeros \n    derivatives['dfgw'] = np.zeros(parameters['fgw'].shape)\n    derivatives['digw'] = np.zeros(parameters['igw'].shape)\n    derivatives['dogw'] = np.zeros(parameters['ogw'].shape)\n    derivatives['dggw'] = np.zeros(parameters['ggw'].shape)\n    \n    #sum up the derivatives for each time step\n    for i in range(1,len(lstm_error_cache)+1):\n        derivatives['dfgw'] += lstm_derivatives['dlstm'+str(i)]['dfgw']\n        derivatives['digw'] += lstm_derivatives['dlstm'+str(i)]['digw']\n        derivatives['dogw'] += lstm_derivatives['dlstm'+str(i)]['dogw']\n        derivatives['dggw'] += lstm_derivatives['dlstm'+str(i)]['dggw']\n    \n    return derivatives,embedding_error_cache","metadata":{"_uuid":"22e0936aad852146820c5c9c37c5bab1d6f4938a","execution":{"iopub.status.busy":"2024-06-14T06:12:55.396324Z","iopub.execute_input":"2024-06-14T06:12:55.396781Z","iopub.status.idle":"2024-06-14T06:12:55.566050Z","shell.execute_reply.started":"2024-06-14T06:12:55.396708Z","shell.execute_reply":"2024-06-14T06:12:55.565073Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Adam Optimizer\nUsing Exponentially Weighted Averages <br>\n* Vdw = beta1 x Vdw + (1-beta1) x (dw)   \n* Sdw = beta2 x Sdw + (1-beta2) x dw^2\n* W = W - learning_rate x ( Vdw/ (sqrt(Sdw)+1e-7) )","metadata":{"_uuid":"92f90ed48cb1d16c58952c06a6605c9bec23ed17"}},{"cell_type":"code","source":"#update the parameters using adam optimizer\n#adam optimization\ndef update_parameters(parameters,derivatives,V,S,t):\n    #get derivatives\n    dfgw = derivatives['dfgw']\n    digw = derivatives['digw']\n    dogw = derivatives['dogw']\n    dggw = derivatives['dggw']\n    dhow = derivatives['dhow']\n    \n    #get parameters\n    fgw = parameters['fgw']\n    igw = parameters['igw']\n    ogw = parameters['ogw']\n    ggw = parameters['ggw']\n    how = parameters['how']\n    \n    #get V parameters\n    vfgw = V['vfgw']\n    vigw = V['vigw']\n    vogw = V['vogw']\n    vggw = V['vggw']\n    vhow = V['vhow']\n    \n    #get S parameters\n    sfgw = S['sfgw']\n    sigw = S['sigw']\n    sogw = S['sogw']\n    sggw = S['sggw']\n    show = S['show']\n    \n    #calculate the V parameters from V and current derivatives\n    vfgw = (beta1*vfgw + (1-beta1)*dfgw)\n    vigw = (beta1*vigw + (1-beta1)*digw)\n    vogw = (beta1*vogw + (1-beta1)*dogw)\n    vggw = (beta1*vggw + (1-beta1)*dggw)\n    vhow = (beta1*vhow + (1-beta1)*dhow)\n    \n    #calculate the S parameters from S and current derivatives\n    sfgw = (beta2*sfgw + (1-beta2)*(dfgw**2))\n    sigw = (beta2*sigw + (1-beta2)*(digw**2))\n    sogw = (beta2*sogw + (1-beta2)*(dogw**2))\n    sggw = (beta2*sggw + (1-beta2)*(dggw**2))\n    show = (beta2*show + (1-beta2)*(dhow**2))\n    \n    #update the parameters\n    fgw = fgw - learning_rate*((vfgw)/(np.sqrt(sfgw) + 1e-6))\n    igw = igw - learning_rate*((vigw)/(np.sqrt(sigw) + 1e-6))\n    ogw = ogw - learning_rate*((vogw)/(np.sqrt(sogw) + 1e-6))\n    ggw = ggw - learning_rate*((vggw)/(np.sqrt(sggw) + 1e-6))\n    how = how - learning_rate*((vhow)/(np.sqrt(show) + 1e-6))\n    \n    #store the new weights\n    parameters['fgw'] = fgw\n    parameters['igw'] = igw\n    parameters['ogw'] = ogw\n    parameters['ggw'] = ggw\n    parameters['how'] = how\n    \n    #store the new V parameters\n    V['vfgw'] = vfgw \n    V['vigw'] = vigw \n    V['vogw'] = vogw \n    V['vggw'] = vggw\n    V['vhow'] = vhow\n    \n    #store the s parameters\n    S['sfgw'] = sfgw \n    S['sigw'] = sigw \n    S['sogw'] = sogw \n    S['sggw'] = sggw\n    S['show'] = show\n    \n    return parameters,V,S    ","metadata":{"_uuid":"cb5eec4b78051a31bf025a418a2536ee9509f82c","execution":{"iopub.status.busy":"2024-06-14T06:12:55.567318Z","iopub.execute_input":"2024-06-14T06:12:55.567721Z","iopub.status.idle":"2024-06-14T06:12:55.844801Z","shell.execute_reply.started":"2024-06-14T06:12:55.567645Z","shell.execute_reply":"2024-06-14T06:12:55.843499Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Update the embeddings","metadata":{"_uuid":"3ff41245f89d4af31bc5e8965cf83525c4694fe5"}},{"cell_type":"code","source":"#update the Embeddings\ndef update_embeddings(embeddings,embedding_error_cache,batch_labels):\n    #to store the embeddings derivatives\n    embedding_derivatives = np.zeros(embeddings.shape)\n    \n    batch_size = batch_labels[0].shape[0]\n    \n    #sum the embedding derivatives for each time step\n    for i in range(len(embedding_error_cache)):\n        embedding_derivatives += np.matmul(batch_labels[i].T,embedding_error_cache['eemb'+str(i)])/batch_size\n    \n    #update the embeddings\n    embeddings = embeddings - learning_rate*embedding_derivatives\n    return embeddings","metadata":{"_uuid":"3d16352990b74ccc2392ac9ee4891b181470f5ff","execution":{"iopub.status.busy":"2024-06-14T06:12:55.846293Z","iopub.execute_input":"2024-06-14T06:12:55.846595Z","iopub.status.idle":"2024-06-14T06:12:55.865920Z","shell.execute_reply.started":"2024-06-14T06:12:55.846546Z","shell.execute_reply":"2024-06-14T06:12:55.864819Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Functions to Initialize the V and S parameters for Adam Optimizer","metadata":{"_uuid":"17f5d44fc69c4694a0d0698588df44a3dce9d9c2"}},{"cell_type":"code","source":"def initialize_V(parameters):\n    Vfgw = np.zeros(parameters['fgw'].shape)\n    Vigw = np.zeros(parameters['igw'].shape)\n    Vogw = np.zeros(parameters['ogw'].shape)\n    Vggw = np.zeros(parameters['ggw'].shape)\n    Vhow = np.zeros(parameters['how'].shape)\n    \n    V = dict()\n    V['vfgw'] = Vfgw\n    V['vigw'] = Vigw\n    V['vogw'] = Vogw\n    V['vggw'] = Vggw\n    V['vhow'] = Vhow\n    return V\n\ndef initialize_S(parameters):\n    Sfgw = np.zeros(parameters['fgw'].shape)\n    Sigw = np.zeros(parameters['igw'].shape)\n    Sogw = np.zeros(parameters['ogw'].shape)\n    Sggw = np.zeros(parameters['ggw'].shape)\n    Show = np.zeros(parameters['how'].shape)\n    \n    S = dict()\n    S['sfgw'] = Sfgw\n    S['sigw'] = Sigw\n    S['sogw'] = Sogw\n    S['sggw'] = Sggw\n    S['show'] = Show\n    return S","metadata":{"_uuid":"c188a724be12aa28481d16fb62e154e77c376570","execution":{"iopub.status.busy":"2024-06-14T06:12:55.867327Z","iopub.execute_input":"2024-06-14T06:12:55.867631Z","iopub.status.idle":"2024-06-14T06:12:55.931140Z","shell.execute_reply.started":"2024-06-14T06:12:55.867582Z","shell.execute_reply":"2024-06-14T06:12:55.929991Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Train Function\n\n1. Initialize Parameters\n2. Forward Propagation\n3. Calculate Loss, Perplexity and Accuracy\n4. Backward Propagation\n5. Update the Parameters and Embeddings\n\nBatch Size = 20\nRepeat the steps 2-5 for each batch!","metadata":{"_uuid":"1530f469e8e81bb04e0f88fe5c40cb3ba06e0735"}},{"cell_type":"code","source":"#train function\ndef train(train_dataset,iters=1000,batch_size=20):\n    #initalize the parameters\n    parameters = initialize_parameters()\n    \n    #initialize the V and S parameters for Adam\n    V = initialize_V(parameters)\n    S = initialize_S(parameters)\n    \n    #generate the random embeddings\n    embeddings = np.random.normal(0,0.01,(len(vocab),input_units))\n    \n    #to store the Loss, Perplexity and Accuracy for each batch\n    J = []\n    P = []\n    A = []\n    \n    \n    for step in range(iters):\n        #get batch dataset\n        index = step%len(train_dataset)\n        batches = train_dataset[index]\n        \n        #forward propagation\n        embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache = forward_propagation(batches,parameters,embeddings)\n        \n        #calculate the loss, perplexity and accuracy\n        perplexity,loss,acc = cal_loss_accuracy(batches,output_cache)\n        \n        #backward propagation\n        derivatives,embedding_error_cache = backward_propagation(batches,embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache,parameters)\n        \n        #update the parameters\n        parameters,V,S = update_parameters(parameters,derivatives,V,S,step)\n        \n        #update the embeddings\n        embeddings = update_embeddings(embeddings,embedding_error_cache,batches)\n        \n        \n        J.append(loss)\n        P.append(perplexity)\n        A.append(acc)\n        \n        #print loss, accuracy and perplexity\n        if(step%1000==0):\n            print(\"For Single Batch :\")\n            print('Step       = {}'.format(step))\n            print('Loss       = {}'.format(round(loss,2)))\n            print('Perplexity = {}'.format(round(perplexity,2)))\n            print('Accuracy   = {}'.format(round(acc*100,2)))\n            print()\n    \n    return embeddings, parameters,J,P,A","metadata":{"_uuid":"a5816f86b11d6ff83e5b8b7717fcee5f8e8bb040","execution":{"iopub.status.busy":"2024-06-14T06:12:55.932882Z","iopub.execute_input":"2024-06-14T06:12:55.933264Z","iopub.status.idle":"2024-06-14T06:12:56.050740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's Train\n* Will take around 5-10 mins on CPU","metadata":{"_uuid":"ea0d72a85111f3d78474c426a9ca570030093c91"}},{"cell_type":"code","source":"embeddings,parameters,J,P,A = train(train_dataset,iters=8001)","metadata":{"_uuid":"fff95d703395812c87fafc59bccc9c867c73c32e","execution":{"iopub.status.busy":"2024-06-14T06:12:56.052028Z","iopub.execute_input":"2024-06-14T06:12:56.052324Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"For Single Batch :\nStep       = 0\nLoss       = 47.05\nPerplexity = 27.0\nAccuracy   = 1.36\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Let's Plot some graphs\n\n*  Plotted average loss of 30 batches, average perplexity of 30 batches, and average accuracy of 30 batches.","metadata":{"_uuid":"cc63678f7f15d413a44fd625af8695daa48e0852"}},{"cell_type":"code","source":"avg_loss = list()\navg_acc = list()\navg_perp = list()\ni = 0\nwhile(i<len(J)):\n    avg_loss.append(np.mean(J[i:i+30]))\n    avg_acc.append(np.mean(A[i:i+30]))\n    avg_perp.append(np.mean(P[i:i+30]))\n    i += 30\n\nplt.plot(list(range(len(avg_loss))),avg_loss)\nplt.xlabel(\"x\")\nplt.ylabel(\"Loss (Avg of 30 batches)\")\nplt.title(\"Loss Graph\")\nplt.show()\n\nplt.plot(list(range(len(avg_perp))),avg_perp)\nplt.xlabel(\"x\")\nplt.ylabel(\"Perplexity (Avg of 30 batches)\")\nplt.title(\"Perplexity Graph\")\nplt.show()\n\nplt.plot(list(range(len(avg_acc))),avg_acc)\nplt.xlabel(\"x\")\nplt.ylabel(\"Accuracy (Avg of 30 batches)\")\nplt.title(\"Accuracy Graph\")\nplt.show()    ","metadata":{"_uuid":"2a08189462b703684c38c188cf8a80cdfc57528f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's make some predictions","metadata":{"_uuid":"bb8738d78ad60f6fe16038b32a29630919e91976"}},{"cell_type":"code","source":"#predict\ndef predict(parameters,embeddings,id_char,vocab_size):\n    #to store some predicted names\n    names = []\n    \n    #predict 20 names\n    for i in range(20):\n        #initial activation_matrix(a0) and cell_matrix(c0)\n        a0 = np.zeros([1,hidden_units],dtype=np.float32)\n        c0 = np.zeros([1,hidden_units],dtype=np.float32)\n\n        #initalize blank name\n        name = ''\n        \n        #make a batch dataset of single char\n        batch_dataset = np.zeros([1,vocab_size])\n        \n        #get random start character\n        index = np.random.randint(0,27,1)[0]\n        \n        #make that index 1.0\n        batch_dataset[0,index] = 1.0\n        \n        #add first char to name\n        name += id_char[index]\n        \n        #get char from id_char dict\n        char = id_char[index]\n        \n        #loop until algo predicts '.'\n        while(char!='.'):\n            #get embeddings\n            batch_dataset = get_embeddings(batch_dataset,embeddings)\n\n            #lstm cell\n            lstm_activations,ct,at = lstm_cell(batch_dataset,a0,c0,parameters)\n\n            #output cell\n            ot = output_cell(at,parameters)\n            \n            #either select random.choice ot np.argmax\n            pred = np.random.choice(27,1,p=ot[0])[0]\n            \n            #get predicted char index\n            #pred = np.argmax(ot)\n                \n            #add char to name\n            name += id_char[pred]\n            \n            char = id_char[pred]\n            \n            #change the batch_dataset to this new predicted char\n            batch_dataset = np.zeros([1,vocab_size])\n            batch_dataset[0,pred] = 1.0\n\n            #update a0 and c0 to new 'at' and 'ct' for next lstm cell\n            a0 = at\n            c0 = ct\n            \n        #append the predicted name to names list\n        names.append(name)\n        \n    return names","metadata":{"_uuid":"09ac989d49560b820a0d23de90707f59ec8700b5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's Predict Names using Argmax","metadata":{"_uuid":"d9c26137d2c08c22efe4d07c0762f4db891c8bc2"}},{"cell_type":"code","source":"predict(parameters,embeddings,id_char,vocab_size)","metadata":{"_uuid":"6ae3c2a1cd42afea050a9612ab07024d2ec5e90d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"00eb5d439a9974174c383b4fc4d8677949650084"}},{"cell_type":"markdown","source":"### Lets predict using Random.Choice","metadata":{"_uuid":"eef286da051a728c715d1124bd37f0b0df106a57"}},{"cell_type":"code","source":"predict(parameters,embeddings,id_char,vocab_size)","metadata":{"_uuid":"e2dc3a10778120b483f8b125f03126a6d9f71561","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"e040a915d2cad0956864608e6c6514aea929bd3e"}},{"cell_type":"markdown","source":"# The END","metadata":{"_uuid":"2c256e2f75fa4cddba0b9ad3be0cbd429d13984a"}}]}